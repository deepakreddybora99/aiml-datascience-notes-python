{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c597053",
   "metadata": {},
   "source": [
    "# <h1 style='text-align:center'> Data Preprocessing for Machine Learning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428b9ea",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Overview\n",
    "Data preprocessing is a critical step in the machine learning pipeline that involves transforming raw, messy data into a clean and structured format suitable for modeling. Proper preprocessing improves model performance, reduces bias, and ensures more accurate predictions.\n",
    "\n",
    "> \"Garbage in, garbage out\" – If the input data is flawed, even the best machine learning algorithms will fail.\n",
    "\n",
    "---\n",
    "\n",
    "## Importance of Data Preprocessing\n",
    "1. **Improves model accuracy** – Clean data helps the model learn patterns effectively.\n",
    "2. **Reduces noise and bias** – Removes irrelevant or misleading information.\n",
    "3. **Speeds up training** – Smaller, cleaner datasets require less computation.\n",
    "4. **Prevents model failure** – Handles missing values, outliers, and inconsistent data.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps in Data Preprocessing\n",
    "\n",
    "### 1. Data Cleaning\n",
    "- **Handling Missing Values:** Remove or fill missing data using mean, median, mode, or predictive models.\n",
    "- **Removing Duplicates:** Duplicate rows or records are eliminated to prevent bias.\n",
    "\n",
    "### 2. Handling Outliers\n",
    "- Outliers can distort learning.\n",
    "- Techniques:\n",
    "  - Z-score method\n",
    "  - Interquartile Range (IQR)\n",
    "  - Clipping extreme values\n",
    "\n",
    "> **Note:** *please find sections of 1 and 2 in Phase_2_Statistics_EDA* \n",
    "\n",
    "### [3. Encoding Categorical Data](#3-data-preprocessing-for-categorical-data-in-machine-learning)\n",
    "- Converts non-numeric categories to numeric values, because machine cannot understand words it can understand numerical values only.\n",
    "\n",
    "### [4. Feature Scaling(Numerical)](#4-numerical-data-preprocessing-for-machine-learning)\n",
    "- Ensures all features are on a similar scale.\n",
    "\n",
    "### [5. Text Data Preprocessing](#5-text-data-preprocessing-for-machine-learning)\n",
    "- Clean, normalize, and convert raw text into structured numerical representations suitable for machine learning models.\n",
    "\n",
    "### [6. Image Data Preprocessing](#6-image-data-preprocessing-for-machine-learning)\n",
    "- Standardize, enhance, and transform raw images into formats and scales suitable for machine learning algorithms.\n",
    "\n",
    "### [7. Feature Selection & Extraction](#7-feature-selection-and-feature-extraction-for-machine-learning)\n",
    "- Reduces irrelevant or redundant features to improve efficiency.\n",
    "\n",
    "### [References](#References)\n",
    "- Official Documentation of Tools\n",
    "---\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "- Scaling before splitting train and test sets  \n",
    "- Applying inconsistent encoding between training and testing data  \n",
    "- Ignoring outliers without analysis  \n",
    "- Removing too much data during cleaning  \n",
    "\n",
    "---\n",
    "\n",
    "## Tools & Libraries\n",
    "- **Pandas** – Data manipulation  \n",
    "- **NumPy** – Numerical operations  \n",
    "- **Scikit-learn** – Preprocessing and ML utilities  \n",
    "- **PIL, OpenCV, Re** - Preprocessing Text and Image Data\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "Data preprocessing transforms raw data into a clean, structured form, enabling machine learning algorithms to learn efficiently. It is one of the most important steps in the ML workflow and often has the largest impact on model performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c1d7e",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for Categorical Data in Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Categorical data represents features that contain **label values rather than numeric values**. Examples include:\n",
    "\n",
    "- Gender: `Male`, `Female`, `Other`\n",
    "- Color: `Red`, `Blue`, `Green`\n",
    "- Product Category: `Electronics`, `Clothing`, `Furniture`\n",
    "\n",
    "Machine learning models require **numerical inputs**, so categorical data must be transformed.\n",
    "\n",
    "---\n",
    "\n",
    "## Categorical Data Types\n",
    "\n",
    "1. **Nominal:** Categories with no intrinsic order.  \n",
    "   Example: `Red`, `Blue`, `Green`\n",
    "\n",
    "2. **Ordinal:** Categories with a meaningful order.  \n",
    "   Example: `Low`, `Medium`, `High`\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing Techniques\n",
    "\n",
    "### 1. Label Encoding\n",
    "Assigns an integer to each category. Suitable for **ordinal data** but can mislead some ML models if applied to nominal data (implies order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['color_encoded'] = le.fit_transform(df['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98945a01",
   "metadata": {},
   "source": [
    "### 2. One-Hot Encoding\n",
    "\n",
    "Creates binary columns for each category. Ideal for **nominal data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "encoded_features = ohe.fit_transform(df[['color']])\n",
    "\n",
    "#pandas\n",
    "df = pd.get_dummies(df, columns=['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a03d9a",
   "metadata": {},
   "source": [
    "### 3. Ordinal Encoding\n",
    "\n",
    "Maps ordered categories to integers reflecting their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a67256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_mapping = [['Low', 'Medium', 'High']]\n",
    "oe = OrdinalEncoder(categories=ordinal_mapping)\n",
    "df['priority_encoded'] = oe.fit_transform(df[['priority']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22916eb6",
   "metadata": {},
   "source": [
    "### 4. Frequency / Count Encoding\n",
    "\n",
    "Replaces categories with the **frequency of their occurrence** in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df['color'].value_counts()\n",
    "df['color_freq'] = df['color'].map(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d89b1",
   "metadata": {},
   "source": [
    "### Implementation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Sample dataset\n",
    "data = {'color': ['Red', 'Blue', 'Green', 'Red'],\n",
    "        'size': ['S', 'M', 'L', 'S'],\n",
    "        'priority': ['Low', 'High', 'Medium', 'Medium']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Label Encoding (color)\n",
    "le = LabelEncoder()\n",
    "df['color_encoded'] = le.fit_transform(df['color'])\n",
    "\n",
    "# One-Hot Encoding (size)\n",
    "df = pd.get_dummies(df, columns=['size'])\n",
    "\n",
    "# Ordinal Encoding (priority)\n",
    "ordinal_mapping = [['Low', 'Medium', 'High']]\n",
    "oe = OrdinalEncoder(categories=ordinal_mapping)\n",
    "df['priority_encoded'] = oe.fit_transform(df[['priority']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e47653",
   "metadata": {},
   "source": [
    "### Tips & Best Practices\n",
    "\n",
    "* Use **one-hot encoding** for nominal features to avoid misleading order relationships.\n",
    "* Use **label or ordinal encoding** for features with meaningful order.\n",
    "* Be cautious of **high cardinality** features; frequency encoding or embedding layers may help.\n",
    "* Avoid **data leakage**: fit encoders on **training data only** and transform test data separately.\n",
    "* Always **handle missing values** before encoding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daed90a",
   "metadata": {},
   "source": [
    "## 4. Numerical Data Preprocessing for Machine Learning\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Numerical features are continuous or discrete numbers in your dataset. Examples include:\n",
    "\n",
    "- Age: 21, 35, 42  \n",
    "- Salary: 40000, 75000, 120000  \n",
    "- Temperature: 36.5, 38.2, 40.1  \n",
    "\n",
    "Preprocessing numerical features is crucial to ensure ML algorithms **perform optimally**.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Issues with Numerical Data\n",
    "\n",
    "1. **Missing Values**: Some entries might be NaN or blank.  \n",
    "2. **Different Scales**: Features can have vastly different ranges.  \n",
    "3. **Outliers**: Extreme values can distort models.  \n",
    "4. **Skewed Distributions**: Non-normal distributions can affect algorithms.  \n",
    "\n",
    "---\n",
    "\n",
    "### Preprocessing Techniques\n",
    "\n",
    "#### 1. Scaling / Normalization\n",
    "\n",
    "**a) Min-Max Scaling**\n",
    "\n",
    "Scales values to range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0dee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[['salary_scaled']] = scaler.fit_transform(df[['salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ebfc9",
   "metadata": {},
   "source": [
    "**b) Standardization (Z-score)**\n",
    "\n",
    "Centers data around 0 with standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['salary_std']] = scaler.fit_transform(df[['salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a030f",
   "metadata": {},
   "source": [
    "#### 2. Feature Transformation\n",
    "\n",
    "* **Log Transformation**: Reduces right-skewed distributions\n",
    "* **Square Root / Cube Root Transformation**: Reduces skewness\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "df['salary_log'] = np.log1p(df['salary'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Discretization / Binning\n",
    "\n",
    "Convert continuous features into bins or categories.\n",
    "\n",
    "```python\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0,18,35,60,100], labels=['Child','Youth','Adult','Senior'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample dataset\n",
    "data = {'age': [25, np.nan, 35, 45, 28],\n",
    "        'salary': [50000, 60000, 75000, 120000, 40000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "# Standardize age\n",
    "scaler = StandardScaler()\n",
    "df['age_std'] = scaler.fit_transform(df[['age']])\n",
    "\n",
    "# Min-Max scale salary\n",
    "minmax = MinMaxScaler()\n",
    "df['salary_scaled'] = minmax.fit_transform(df[['salary']])\n",
    "\n",
    "# Log transform salary\n",
    "df['salary_log'] = np.log1p(df['salary'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c697f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Tips & Best Practices\n",
    "\n",
    "* Always **fit scalers and imputers on training data** and transform test data separately.\n",
    "* Use **StandardScaler for algorithms sensitive to variance** (e.g., SVM, KNN).\n",
    "* Use **MinMaxScaler for algorithms requiring bounded input** (e.g., Neural Networks).\n",
    "* Treat **outliers carefully**; sometimes they carry important information.\n",
    "* Check the **distribution** of your data before choosing transformation methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501509f",
   "metadata": {},
   "source": [
    "## 5. Text Data Preprocessing for Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Text data, unlike numeric data, is **unstructured**. Examples include:\n",
    "\n",
    "- Reviews: `\"The product is amazing!\"`  \n",
    "- Tweets: `\"I love this movie #awesome\"`  \n",
    "- Emails: `\"Dear user, your account is updated\"`\n",
    "\n",
    "Raw text cannot be directly fed into ML models. Preprocessing is necessary to **clean, normalize, and convert text into numerical features**.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Issues in Text Data\n",
    "\n",
    "1. **Inconsistent casing**: `\"Machine Learning\"` vs `\"machine learning\"`  \n",
    "2. **Punctuation, numbers, symbols**  \n",
    "3. **Stopwords**: Common words like \"the\", \"is\", \"and\" add noise  \n",
    "4. **Different word forms**: `\"run\"`, `\"running\"`, `\"ran\"`  \n",
    "5. **High dimensionality** in vectorized text  \n",
    "\n",
    "---\n",
    "\n",
    "### Preprocessing Techniques\n",
    "\n",
    "### 1. Lowercasing\n",
    "Convert all text to lowercase for consistency.\n",
    "\n",
    "```python\n",
    "text = \"Machine Learning is FUN!\"\n",
    "text = text.lower()\n",
    "# Output: \"machine learning is fun!\"\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Removing Punctuation, Numbers, and Whitespace\n",
    "\n",
    "Remove characters that don’t contribute to meaning.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"I love ML 101!!!\"\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "# Output: \"I love ML\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Tokenization\n",
    "\n",
    "Split text into words or subwords.\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"I love machine learning\")\n",
    "# Output: ['I', 'love', 'machine', 'learning']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Stopword Removal\n",
    "\n",
    "Remove common words that don’t carry significant meaning.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if w.lower() not in stop_words]\n",
    "# Output: ['love', 'machine', 'learning']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Stemming and Lemmatization\n",
    "\n",
    "Reduce words to their root form.\n",
    "\n",
    "**Stemming:**\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in tokens]\n",
    "# Output: ['love', 'machin', 'learn']\n",
    "```\n",
    "\n",
    "**Lemmatization:**\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "# Output: ['love', 'machine', 'learning']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Vectorization\n",
    "\n",
    "Convert text into numerical representations:\n",
    "\n",
    "* **Bag of Words (BoW)**\n",
    "* **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "* **Word Embeddings (Word2Vec, GloVe, FastText)**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"I love machine learning\", \"ML is fun\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Handling Out-of-Vocabulary Words\n",
    "\n",
    "For embeddings or NLP models, handle unknown words by:\n",
    "\n",
    "* Assigning a special token `<UNK>`\n",
    "* Using subword tokenization (BPE, WordPiece)\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Example\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "corpus = [\"I love machine learning!\", \"Natural Language Processing is amazing.\"]\n",
    "\n",
    "cleaned_corpus = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for doc in corpus:\n",
    "    doc = doc.lower()  # Lowercase\n",
    "    doc = re.sub(r'[^a-z\\s]', '', doc)  # Remove punctuation\n",
    "    tokens = word_tokenize(doc)  # Tokenization\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]  # Lemmatization + Stopwords\n",
    "    cleaned_corpus.append(\" \".join(tokens))\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(cleaned_corpus)\n",
    "\n",
    "print(cleaned_corpus)\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Tips & Best Practices\n",
    "\n",
    "* Always **clean text** before vectorization.\n",
    "* Use **lemmatization** over stemming for better semantic understanding, due after stemming words does not having meaning\n",
    "* For deep learning, **word embeddings** often outperform BoW or TF-IDF.\n",
    "* Handle **special characters, emojis, and URLs** in social media text.\n",
    "* Consider **n-grams** for capturing context.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00bc7a",
   "metadata": {},
   "source": [
    "## 6. Image Data Preprocessing for Machine Learning\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Image data is **high-dimensional and unstructured**. Examples include:\n",
    "\n",
    "- Photographs of objects or faces  \n",
    "- Medical images (X-rays, MRIs)  \n",
    "- Handwritten digits (MNIST dataset)  \n",
    "\n",
    "Raw image pixels often require preprocessing before feeding into ML models.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Issues in Image Data\n",
    "\n",
    "1. **Different sizes and resolutions**  \n",
    "2. **Different color scales** (RGB, grayscale)  \n",
    "3. **High dimensionality**  \n",
    "4. **Noise or artifacts**  \n",
    "5. **Limited dataset size**\n",
    "\n",
    "---\n",
    "\n",
    "### Preprocessing Techniques\n",
    "\n",
    "#### 1. Resizing\n",
    "Resize images to a uniform size for consistent input dimensions.\n",
    "\n",
    "```python\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open('image.jpg')\n",
    "resized_img = img.resize((128, 128))\n",
    "resized_array = np.array(resized_img)\n",
    "````\n",
    "\n",
    "Or using OpenCV:\n",
    "\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('image.jpg')\n",
    "resized_img = cv2.resize(img, (128, 128))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Normalization / Scaling\n",
    "\n",
    "Scale pixel values to a standard range, usually `[0,1]`.\n",
    "\n",
    "```python\n",
    "normalized_img = resized_img / 255.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Data Augmentation\n",
    "\n",
    "Generate new images from existing ones to increase dataset size:\n",
    "\n",
    "* Rotation\n",
    "* Flipping\n",
    "* Translation\n",
    "* Brightness adjustment\n",
    "\n",
    "```python\n",
    "# Rotation example using OpenCV\n",
    "(h, w) = resized_img.shape[:2]\n",
    "center = (w // 2, h // 2)\n",
    "matrix = cv2.getRotationMatrix2D(center, angle=30, scale=1.0)\n",
    "rotated_img = cv2.warpAffine(resized_img, matrix, (w, h))\n",
    "\n",
    "# Horizontal flip\n",
    "flipped_img = cv2.flip(resized_img, 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Grayscale Conversion\n",
    "\n",
    "Convert RGB images to grayscale to reduce dimensionality if color is not important.\n",
    "\n",
    "```python\n",
    "gray_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "```\n",
    "\n",
    "Or using PIL:\n",
    "\n",
    "```python\n",
    "gray_img = img.convert('L')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Denoising / Filtering\n",
    "\n",
    "Reduce noise in images using filters:\n",
    "\n",
    "```python\n",
    "denoised_img = cv2.GaussianBlur(resized_img, (5, 5), 0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Image Flattening\n",
    "\n",
    "Flatten 2D images into 1D arrays for traditional ML models like SVM or Random Forest.\n",
    "\n",
    "```python\n",
    "flattened_img = resized_img.flatten()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Example\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load image using PIL\n",
    "img = Image.open('image.jpg')\n",
    "\n",
    "# Resize\n",
    "resized_img = img.resize((128, 128))\n",
    "resized_array = np.array(resized_img)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray_img = img.convert('L')\n",
    "gray_array = np.array(gray_img)\n",
    "\n",
    "# Normalize\n",
    "normalized_img = resized_array / 255.0\n",
    "\n",
    "# Data augmentation: rotation\n",
    "(h, w) = resized_array.shape[:2]\n",
    "center = (w // 2, h // 2)\n",
    "matrix = cv2.getRotationMatrix2D(center, angle=30, scale=1.0)\n",
    "rotated_img = cv2.warpAffine(resized_array, matrix, (w, h))\n",
    "\n",
    "# Horizontal flip\n",
    "flipped_img = cv2.flip(resized_array, 1)\n",
    "\n",
    "# Flatten for ML\n",
    "flattened_img = normalized_img.flatten()\n",
    "\n",
    "print(\"Resized shape:\", resized_array.shape)\n",
    "print(\"Grayscale shape:\", gray_array.shape)\n",
    "print(\"Flattened shape:\", flattened_img.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Tips & Best Practices\n",
    "\n",
    "* Always **resize images** to the same dimensions for batch processing.\n",
    "* Normalize pixel values to **improve convergence** of ML models.\n",
    "* Use **data augmentation** to reduce overfitting in small datasets.\n",
    "* Use **grayscale** only when color information is unnecessary.\n",
    "* For CNNs, keep **3D shapes** (`height x width x channels`) instead of flattening.\n",
    "* Save preprocessed images using PIL or OpenCV to avoid repeated computation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf448d",
   "metadata": {},
   "source": [
    "## 7. Feature Selection and Feature Extraction for Machine Learning\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Features (or variables) are the **inputs** to machine learning models. Not all features are equally informative.  \n",
    "\n",
    "- **Feature Selection:** Choose the most relevant features from the existing dataset.  \n",
    "- **Feature Extraction:** Transform the data into a lower-dimensional space while preserving important information.  \n",
    "\n",
    "Proper feature selection and extraction improve **model accuracy, reduce overfitting, and decrease training time**.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Feature Selection Objective:**  \n",
    "  *\"Identify and retain the most relevant features to improve model performance and interpretability.\"*\n",
    "\n",
    "- **Feature Extraction Objective:**  \n",
    "  *\"Transform original features into a lower-dimensional representation that preserves essential information for modeling.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "#### 1. Filter Methods\n",
    "Use statistical measures to score features independently of the model.\n",
    "\n",
    "- **Techniques:** Correlation, Chi-square, ANOVA, Mutual Information\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X_new = SelectKBest(chi2, k=5).fit_transform(X, y)\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Wrapper Methods\n",
    "\n",
    "Use a predictive model to evaluate feature subsets.\n",
    "\n",
    "* **Techniques:** Recursive Feature Elimination (RFE), Sequential Feature Selection\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Embedded Methods\n",
    "\n",
    "Feature selection is built into the model training.\n",
    "\n",
    "* **Techniques:** Lasso Regression (L1), Tree-based feature importance\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.01)\n",
    "model.fit(X, y)\n",
    "selected_features = X.columns[model.coef_ != 0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "#### 1. Principal Component Analysis (PCA)\n",
    "\n",
    "Transforms features into **uncorrelated components** while retaining maximum variance.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "#### 2. Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Projects features to maximize **class separability**. Often used in classification tasks.\n",
    "\n",
    "```python\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "```\n",
    "\n",
    "#### 3. t-SNE / UMAP\n",
    "\n",
    "Non-linear dimensionality reduction for **visualization and clustering**.\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Example\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Feature Selection: top 2 features\n",
    "X_selected = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "\n",
    "# Feature Extraction: PCA to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Selected Features Shape:\", X_selected.shape)\n",
    "print(\"PCA Features Shape:\", X_pca.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Tips & Best Practices\n",
    "\n",
    "* **Feature Selection** reduces noise and improves interpretability.\n",
    "* **Feature Extraction** is useful for high-dimensional datasets (images, text).\n",
    "* Use **PCA** when you want to preserve variance, **LDA** when class separation is important.\n",
    "* Combine **feature selection and extraction** for optimal results.\n",
    "* Avoid **leakage**: fit transformations only on training data, then apply to test data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e53b61",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Scikit-learn preprocessing documentation](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "2. [One-hot encoding explained](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)\n",
    "3. [Handling categorical data in ML](https://towardsdatascience.com/encoding-categorical-features-21a2651a065c)\n",
    "4. [Data preprocessing in ML](https://towardsdatascience.com/data-preprocessing-for-machine-learning-3b61b4f94d7f)\n",
    "5. [Feature scaling techniques](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)\n",
    "6. [NLTK Documentation](https://www.nltk.org/)\n",
    "7. [Scikit-learn Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "8. [Text Preprocessing in NLP](https://towardsdatascience.com/text-preprocessing-for-nlp-5e0f746abf1)\n",
    "9. [Word Embeddings](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "10. [OpenCV Documentation](https://docs.opencv.org/)\n",
    "11. [Pillow (PIL) Documentation](https://pillow.readthedocs.io/en/stable/)\n",
    "12. [Image Preprocessing Techniques](https://towardsdatascience.com/image-preprocessing-for-deep-learning-64113f78588f)\n",
    "13. [Data Augmentation in OpenCV](https://learnopencv.com/data-augmentation-techniques-for-deep-learning/)\n",
    "14. [Scikit-learn Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "15. [PCA Tutorial](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
    "16. [LDA in Python](https://scikit-learn.org/stable/modules/lda_qda.html)\n",
    "17. [t-SNE Guide](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# <center> *End of Topic* </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "note_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
