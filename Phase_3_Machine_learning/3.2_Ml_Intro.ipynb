{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13c0dca",
   "metadata": {},
   "source": [
    "# <h1 style='text-align:center'>**Introduction to Machine Learning (ML)**</h1>\n",
    "---\n",
    "---\n",
    "\n",
    "#### **What is Machine Learning?**\n",
    "\n",
    "Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on developing algorithms and models that enable computers to **learn from data and make predictions or decisions without being explicitly programmed**. Unlike traditional programming, where rules are manually coded, ML systems improve automatically as they are exposed to more data.\n",
    "\n",
    "In simpler terms:\n",
    "\n",
    "> Machine Learning allows computers to **learn patterns from data** and **take actions** based on those patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts in Machine Learning**\n",
    "\n",
    "1. **Data**\n",
    "\n",
    "   * ML systems rely heavily on data—this could be images, text, numbers, or sensor readings.\n",
    "   * The quality and quantity of data greatly affect the performance of the ML model.\n",
    "\n",
    "2. **Features**\n",
    "\n",
    "   * Features are the **individual measurable properties or characteristics** of the data.\n",
    "   * Example: In predicting house prices, features could include the number of bedrooms, location, and square footage.\n",
    "\n",
    "3. **Model**\n",
    "\n",
    "   * A model is the mathematical representation of patterns learned from data.\n",
    "   * It maps input data (features) to an output (prediction or classification).\n",
    "\n",
    "4. **Training**\n",
    "\n",
    "   * Training is the process of feeding data to the ML algorithm so it can **learn patterns**.\n",
    "   * The algorithm adjusts its internal parameters to minimize errors.\n",
    "\n",
    "5. **Prediction/Inference**\n",
    "\n",
    "   * Once trained, the model can make predictions on **new, unseen data**.\n",
    "\n",
    "6. **Evaluation**\n",
    "\n",
    "   * ML models are evaluated using metrics like **accuracy, precision, recall, or mean squared error**, depending on the type of problem.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of Machine Learning**\n",
    "\n",
    "* **Healthcare:** Disease prediction, medical image analysis.\n",
    "* **Finance:** Fraud detection, stock market predictions.\n",
    "* **E-commerce:** Recommendation systems (e.g., Netflix, Amazon).\n",
    "* **Autonomous Systems:** Self-driving cars, drones.\n",
    "* **Natural Language Processing:** Chatbots, translation, sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "* Machine Learning enables computers to **learn from data** instead of being explicitly programmed.\n",
    "* The **type of ML** depends on whether the data is labeled and the type of problem being solved.\n",
    "* ML is widely used in real-world applications and is transforming industries globally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cecc8d9",
   "metadata": {},
   "source": [
    "## 1. Understanding AI, ML, DL, GenAI, and LLMs\n",
    "\n",
    "This README provides a concise overview of key AI-related technologies and how they relate to each other.\n",
    "\n",
    "### Table: Differences\n",
    "\n",
    "| Term | Definition | Subset of | How it Works | Examples | Key Point |\n",
    "|------|------------|-----------|--------------|---------|-----------|\n",
    "| **Artificial Intelligence (AI)** | Broad science of making machines perform tasks that require human intelligence | – | Rule-based systems, logic, or data-driven methods | Chess-playing AI, Siri, Fraud detection | Umbrella term for all intelligent machines |\n",
    "| **Machine Learning (ML)** | AI that **learns from data** and improves automatically | AI | Algorithms learn patterns from data to make predictions or decisions | Spam filters, Recommendation systems, Price prediction | AI that **learns from data** |\n",
    "| **Deep Learning (DL)** | ML that uses **deep neural networks** to learn complex patterns from large data | ML | Multi-layer neural networks extract features automatically from data | Image recognition, Speech recognition, Self-driving cars | ML for **big and complex data** |\n",
    "| **Generative AI (GenAI)** | AI that **creates new content** (text, images, audio, code) | AI / ML / DL | Models generate new outputs resembling training data | ChatGPT, DALL·E, GitHub Copilot | AI focused on **creation**, not just prediction |\n",
    "| **Large Language Models (LLMs)** | DL models trained on massive text to **understand & generate human-like language** | DL / GenAI | Learn patterns of language from huge text corpora | GPT-4/5, LLaMA, Claude | GenAI specialized for **language tasks** |\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **AI**: The broad field of machine intelligence.  \n",
    "- **ML**: AI that learns from data.  \n",
    "- **DL**: ML using deep neural networks.  \n",
    "- **GenAI**: AI that generates new content.  \n",
    "- **LLMs**: GenAI specialized for language understanding and generation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443eda9",
   "metadata": {},
   "source": [
    "## 2. Types of Machine Learning\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4299b8f",
   "metadata": {},
   "source": [
    "### **2.1 Supervised Machine Learning**\n",
    "\n",
    "#### **Definition**\n",
    "\n",
    "Supervised Machine Learning (SML) is a type of ML where the model **learns from labeled data**. Each input (feature) in the dataset has a **corresponding output (label)**. The goal is to learn a **mapping from input to output** so that the model can predict outputs for **new, unseen data**.\n",
    "\n",
    "* **Input → Output**\n",
    "* Data must be **labeled** (e.g., “spam” or “not spam”).\n",
    "\n",
    "---\n",
    "\n",
    "#### **How it Works**\n",
    "\n",
    "1. **Collect labeled data** – Data with known inputs and outputs.\n",
    "2. **Split data** – Usually into training and testing sets.\n",
    "3. **Choose an algorithm** – E.g., Linear Regression, Decision Tree, etc.\n",
    "4. **Train the model** – Algorithm learns patterns in the training data.\n",
    "5. **Test/Evaluate** – Check how well the model predicts on unseen test data using metrics like accuracy, RMSE, or F1-score.\n",
    "6. **Prediction** – Use the trained model to predict new data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Types of Supervised Learning**\n",
    "\n",
    "1. **Regression** – Predict **continuous values**.\n",
    "\n",
    "   * Example: Predicting house prices, temperature, stock prices.\n",
    "2. **Classification** – Predict **discrete categories/classes**.\n",
    "\n",
    "   * Example: Email spam detection, disease diagnosis (yes/no).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Common Algorithms**\n",
    "\n",
    "| Algorithm                        | Type                      | Description                                              | Example Use Case                              |\n",
    "| -------------------------------- | ------------------------- | -------------------------------------------------------- | --------------------------------------------- |\n",
    "| **Linear Regression**            | Regression                | Predicts continuous output by fitting a line to data     | Predict house prices based on size, location  |\n",
    "| **Logistic Regression**          | Classification            | Predicts probability of a categorical outcome            | Spam detection, disease prediction            |\n",
    "| **Decision Tree**                | Regression/Classification | Splits data into branches based on feature decisions     | Customer churn prediction                     |\n",
    "| **Random Forest**                | Regression/Classification | Ensemble of decision trees for better accuracy           | Fraud detection, stock prediction             |\n",
    "| **Support Vector Machine (SVM)** | Regression/Classification | Finds the best boundary (hyperplane) to separate classes | Handwriting recognition, credit scoring                       |\n",
    "| **K-Nearest Neighbors (KNN)**    | Regression/Classification | Predicts output based on nearest neighbors               | Recommender systems, credit scoring           |\n",
    "| **Naive Bayes**                  | Classification            | Uses probability based on Bayes’ theorem                 | Email spam classification, sentiment analysis |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Supervised Learning**\n",
    "\n",
    "| Domain      | Problem                                  | Type           | Algorithm Example                           |\n",
    "| ----------- | ---------------------------------------- | -------------- | ------------------------------------------- |\n",
    "| E-commerce  | Predict if a customer will buy a product | Classification | Logistic Regression, Random Forest          |\n",
    "| Healthcare  | Predict patient’s blood pressure         | Regression     | Linear Regression, Random Forest Regression |\n",
    "| Finance     | Detect fraudulent transactions           | Classification | Decision Tree, SVM, Random Forest           |\n",
    "| Marketing   | Predict customer churn                   | Classification | KNN, Logistic Regression                    |\n",
    "| Real Estate | Predict house prices                     | Regression     | Linear Regression, Random Forest            |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Points**\n",
    "\n",
    "* Requires **labeled data**.\n",
    "* Divided into **regression (continuous)** and **classification (categorical)** problems.\n",
    "* Performance depends on **data quality, feature selection, and algorithm choice**.\n",
    "* Often evaluated using metrics like **accuracy, precision, recall, F1-score (classification)** or **MSE/RMSE (regression)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e7c16",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **2.2 Unsupervised Machine Learning (UML)**\n",
    "\n",
    "#### **Definition**\n",
    "\n",
    "Unsupervised Machine Learning is a type of ML where the model learns patterns from **unlabeled data**. Unlike supervised learning, there are **no predefined outputs or labels**. The goal is to find **hidden structures, patterns, or relationships** in the data.\n",
    "\n",
    "* **Input → No explicit Output**\n",
    "* Focuses on **clustering, grouping, and reducing dimensionality**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How it Works**\n",
    "\n",
    "1. **Collect unlabeled data** – Data has features but no target labels.\n",
    "2. **Choose an algorithm** – E.g., K-Means, Hierarchical Clustering, PCA.\n",
    "3. **Train the model** – Algorithm finds patterns, clusters, or reduces dimensionality.\n",
    "4. **Interpret results** – Use patterns for insights, visualization, or preprocessing for other ML tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Types of Unsupervised Learning**\n",
    "\n",
    "| Type                          | Purpose                              | Description                                        | Example Use Case                               |\n",
    "| ----------------------------- | ------------------------------------ | -------------------------------------------------- | ---------------------------------------------- |\n",
    "| **Clustering**                | Group similar data points            | Divides data into clusters based on similarity     | Customer segmentation, market analysis         |\n",
    "| **Dimensionality Reduction**  | Reduce number of features            | Compresses data while retaining important patterns | PCA for visualization, noise reduction         |\n",
    "| **Anomaly Detection**         | Detect unusual data points           | Identifies outliers or rare events                 | Fraud detection, fault detection in machines   |\n",
    "| **Association Rule Learning** | Find relationships between variables | Identifies frequent patterns or co-occurrences     | Market basket analysis, recommendation systems |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Common Algorithms**\n",
    "\n",
    "| Algorithm                              | Type                     | Description                                           | Example Use Case                                  |\n",
    "| -------------------------------------- | ------------------------ | ----------------------------------------------------- | ------------------------------------------------- |\n",
    "| **K-Means Clustering**                 | Clustering               | Divides data into k clusters based on distance        | Customer segmentation, image compression          |\n",
    "| **Hierarchical Clustering**            | Clustering               | Builds a tree of clusters (dendrogram)                | Gene expression analysis, social network grouping |\n",
    "| **DBSCAN**                             | Clustering               | Density-based clustering for irregular shapes         | Detecting fraud, geospatial analysis              |\n",
    "| **Principal Component Analysis (PCA)** | Dimensionality Reduction | Reduces features while preserving variance            | Visualizing high-dimensional data                 |\n",
    "| **t-SNE / UMAP**                       | Dimensionality Reduction | Maps high-dimensional data to 2D/3D for visualization | Pattern discovery in datasets                     |\n",
    "| **Apriori Algorithm**                  | Association Rule         | Finds frequent itemsets and association rules         | Market basket analysis, recommendation systems    |\n",
    "| **Isolation Forest**                   | Anomaly Detection        | Detects anomalies by isolating data points            | Fraud detection, network intrusion detection      |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Unsupervised Learning**\n",
    "\n",
    "| Domain       | Problem                                       | Type                      | Algorithm Example        |\n",
    "| ------------ | --------------------------------------------- | ------------------------- | ------------------------ |\n",
    "| Retail       | Segment customers based on buying habits      | Clustering                | K-Means, Hierarchical    |\n",
    "| Finance      | Detect unusual transactions                   | Anomaly Detection         | Isolation Forest, DBSCAN |\n",
    "| Social Media | Group similar users or posts                  | Clustering                | K-Means, DBSCAN          |\n",
    "| Healthcare   | Reduce dimensionality of gene expression data | Dimensionality Reduction  | PCA, t-SNE               |\n",
    "| Marketing    | Discover frequently bought product sets       | Association Rule Learning | Apriori Algorithm        |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Points**\n",
    "\n",
    "* Works with **unlabeled data**.\n",
    "* Focuses on **finding hidden patterns or structures**.\n",
    "* Often used for **exploratory data analysis (EDA)**.\n",
    "* Clustering, dimensionality reduction, and anomaly detection are common applications.\n",
    "* Helps **discover insights** that are not explicitly labeled in the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad55c51",
   "metadata": {},
   "source": [
    "\n",
    "### **2.3 Semi-Supervised Learning (SSL)**\n",
    "\n",
    "#### **Definition**\n",
    "\n",
    "Semi-Supervised Learning is a type of ML that uses **a small amount of labeled data and a large amount of unlabeled data** for training. It combines aspects of **supervised** and **unsupervised learning** to improve learning efficiency when labeling data is expensive or time-consuming.\n",
    "\n",
    "* **Input → Partially labeled Output**\n",
    "* Goal: **Leverage unlabeled data** to improve model accuracy while using minimal labeled data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How it Works**\n",
    "\n",
    "1. **Collect data** – Most data is unlabeled; only a small portion is labeled.\n",
    "2. **Train model with labeled data** – Start with supervised learning on labeled data.\n",
    "3. **Use unlabeled data** – The model predicts labels for unlabeled data and iteratively improves learning.\n",
    "4. **Refine model** – Repeat the process to improve accuracy and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Types of Semi-Supervised Learning Methods**\n",
    "\n",
    "| Method                  | Description                                                                                     | Example Use Case                                |\n",
    "| ----------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------- |\n",
    "| **Self-training**       | Model is trained on labeled data, then predicts labels for unlabeled data iteratively           | Text classification, email filtering            |\n",
    "| **Co-training**         | Two models are trained on different feature sets and label unlabeled data for each other        | Web page classification, multi-view learning    |\n",
    "| **Graph-based methods** | Uses a graph to represent data points and propagates labels through connected nodes             | Social network analysis, recommendation systems |\n",
    "| **Generative models**   | Uses unsupervised models (like autoencoders) to learn data structure and improve classification | Image recognition with limited labeled data     |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Common Algorithms**\n",
    "\n",
    "| Algorithm                         | Type           | Description                                                      | Example Use Case                          |\n",
    "| --------------------------------- | -------------- | ---------------------------------------------------------------- | ----------------------------------------- |\n",
    "| **Self-training with SVM**        | Classification | Start with labeled data, predict unlabeled data, retrain         | Spam detection, sentiment analysis        |\n",
    "| **Label Propagation**             | Graph-based    | Labels spread from labeled nodes to connected unlabeled nodes    | Social network node classification        |\n",
    "| **Semi-supervised K-Means**       | Clustering     | Uses labeled points to guide clustering of unlabeled data        | Customer segmentation with partial labels |\n",
    "| **Generative Models (VAE, GANs)** | Generative     | Learn underlying data distribution and improve label predictions | Image classification with few labels      |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Semi-Supervised Learning**\n",
    "\n",
    "| Domain              | Problem                                             | Method/Algorithm             |\n",
    "| ------------------- | --------------------------------------------------- | ---------------------------- |\n",
    "| Healthcare          | Classify medical images with few labeled scans      | Self-training with CNN       |\n",
    "| Finance             | Fraud detection with limited labeled transactions   | Label Propagation            |\n",
    "| E-commerce          | Product categorization with partially labeled items | Semi-supervised K-Means      |\n",
    "| NLP                 | Sentiment analysis on social media posts            | Self-training with SVM       |\n",
    "| Autonomous Vehicles | Object detection with limited annotated images      | Generative models (VAE/GANs) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Points**\n",
    "\n",
    "* **Uses both labeled and unlabeled data**, making it cost-effective.\n",
    "* Bridges **supervised and unsupervised learning**.\n",
    "* Ideal when **labeling is expensive or time-consuming**.\n",
    "* Improves performance compared to using only small labeled datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f92d4",
   "metadata": {},
   "source": [
    "### **2.4 Reinforcement Learning (RL)**\n",
    "\n",
    "#### **Definition**\n",
    "Reinforcement Learning is a type of machine learning where an **agent learns to make decisions by interacting with an environment**. The agent takes **actions** and receives **rewards or penalties** based on the outcome, learning over time to **maximize cumulative rewards**.  \n",
    "\n",
    "- **Input → Actions → Feedback (Reward/Penalty) → Learning**\n",
    "- RL is **trial-and-error based learning**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **How it Works**\n",
    "\n",
    "1. **Agent** – The learner or decision-maker.  \n",
    "2. **Environment** – The system or world the agent interacts with.  \n",
    "3. **State (s)** – The current situation of the agent in the environment.  \n",
    "4. **Action (a)** – The choice the agent makes in the current state.  \n",
    "5. **Reward (r)** – Feedback from the environment for the action.  \n",
    "6. **Policy (π)** – Strategy the agent uses to decide actions.  \n",
    "7. **Goal** – Learn a policy that **maximizes cumulative rewards** over time.  \n",
    "\n",
    "**Workflow:**  \n",
    "```\n",
    "State → Action → Environment → Reward + New State → Agent updates Policy → Repeat\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Types of Reinforcement Learning**\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Positive Reinforcement** | Agent is rewarded for good actions | Game AI gets points for winning |\n",
    "| **Negative Reinforcement** | Agent avoids actions that give penalties | Robot learns to avoid obstacles |\n",
    "| **Model-Based RL** | Agent builds a model of the environment and plans ahead | Chess AI predicting future moves |\n",
    "| **Model-Free RL** | Agent learns from trial-and-error without a model | Q-Learning, Policy Gradient |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Common RL Algorithms**\n",
    "\n",
    "| Algorithm | Type | Description | Example Use Case |\n",
    "|-----------|------|-------------|----------------|\n",
    "| **Q-Learning** | Model-free | Learns value of action in each state (Q-table) | Grid-world navigation, simple games |\n",
    "| **Deep Q-Network (DQN)** | Model-free, Deep RL | Uses neural networks to approximate Q-values | Atari games, robotics control |\n",
    "| **SARSA** | Model-free | Updates action-values based on the next action actually taken | Robot navigation |\n",
    "| **Policy Gradient** | Model-free | Directly learns the policy function | Continuous action tasks, robotics |\n",
    "| **Actor-Critic** | Hybrid | Combines policy-based and value-based learning | Complex simulations, OpenAI Gym environments |\n",
    "| **Monte Carlo Methods** | Model-free | Learns value functions by averaging returns over episodes | Board games, stochastic environments |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Reinforcement Learning**\n",
    "\n",
    "| Domain | Problem | Algorithm Example |\n",
    "|--------|---------|-----------------|\n",
    "| Gaming | Train AI to play Atari games | DQN, Q-Learning |\n",
    "| Robotics | Teach a robot to walk | Policy Gradient, Actor-Critic |\n",
    "| Finance | Optimize stock trading strategy | Q-Learning, Monte Carlo RL |\n",
    "| Autonomous Vehicles | Learn to drive safely | Actor-Critic, DDPG |\n",
    "| Resource Management | Optimize data center energy usage | Reinforcement-based optimization |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Points**\n",
    "- RL is **trial-and-error based learning**.  \n",
    "- Focuses on **learning policies** to maximize cumulative reward.  \n",
    "- Works with **dynamic environments**, unlike supervised learning.  \n",
    "- Requires balancing **exploration (trying new actions)** and **exploitation (using known best actions)**.  \n",
    "- Often used in **games, robotics, autonomous systems, and real-time decision-making**.  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46835de9",
   "metadata": {},
   "source": [
    "### **2.1.1Regression in Machine Learning**\n",
    "\n",
    "#### **Definition**\n",
    "\n",
    "Regression is a type of **Supervised Learning** used to predict **continuous numerical values** based on one or more input features.\n",
    "\n",
    "* **Input → Continuous Output**\n",
    "* Goal: Find a **relationship between independent variables (features) and a dependent variable (target)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How Regression Works**\n",
    "\n",
    "1. **Collect labeled data** – Features (inputs) and target (output).\n",
    "2. **Choose a regression algorithm** – E.g., Linear Regression, Decision Tree Regression.\n",
    "3. **Train the model** – Learn the relationship between inputs and outputs.\n",
    "4. **Predict on new data** – Estimate continuous target values.\n",
    "5. **Evaluate performance** – Using metrics like **Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), R² score**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Regression Algorithms**\n",
    "\n",
    "| Type                                | Description                                                                      | Example Use Case                                        |\n",
    "| ----------------------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------- |\n",
    "| **Linear Regression**               | Predicts output as a **linear combination** of input features                    | Predict house prices based on size, bedrooms            |\n",
    "| **Polynomial Regression**           | Fits a **polynomial curve** to model non-linear relationships                    | Predicting growth trends or temperature patterns        |\n",
    "| **Ridge Regression**                | Linear regression with **L2 regularization** to reduce overfitting               | Predicting sales with many correlated features          |\n",
    "| **Lasso Regression**                | Linear regression with **L1 regularization** to shrink some coefficients to zero | Feature selection + prediction in high-dimensional data |\n",
    "| **Logistic Regression**             | Actually a **classification** algorithm, used for binary outcomes                | Predicting spam/not-spam emails                         |\n",
    "| **Decision Tree Regression**        | Uses tree-based splits to predict continuous values                              | Predicting customer spending                            |\n",
    "| **Random Forest Regression**        | Ensemble of decision trees for robust regression                                 | Stock price prediction, energy consumption              |\n",
    "| **Support Vector Regression (SVR)** | Finds a hyperplane within a threshold to predict values                          | Predicting temperature, housing prices                  |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Regression**\n",
    "\n",
    "| Domain      | Problem                                     | Algorithm Example                               |\n",
    "| ----------- | ------------------------------------------- | ----------------------------------------------- |\n",
    "| Real Estate | Predict house prices                        | Linear Regression, Random Forest Regression     |\n",
    "| Finance     | Forecast stock prices                       | SVR, Decision Tree Regression                   |\n",
    "| Healthcare  | Predict blood pressure or cholesterol level | Linear Regression, Ridge/Lasso Regression       |\n",
    "| Marketing   | Predict sales based on ad spend             | Polynomial Regression, Random Forest Regression |\n",
    "| Weather     | Predict temperature or rainfall             | SVR, Linear/Polynomial Regression               |\n",
    "\n",
    "---\n",
    "\n",
    "### **Regression Evaluation Metrics**\n",
    "\n",
    "Regression metrics measure the **difference between predicted values and actual values**. The choice of metric depends on the problem and sensitivity to errors.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Mean Absolute Error (MAE)**\n",
    "\n",
    "* **Definition:** Average of the **absolute differences** between predicted and actual values.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "  $$\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Easy to interpret.\n",
    "  * Treats all errors equally.\n",
    "* **Use Case:** When you want a simple, robust measure of average error.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Mean Squared Error (MSE)**\n",
    "\n",
    "* **Definition:** Average of the **squared differences** between predicted and actual values.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Penalizes **larger errors more heavily**.\n",
    "  * Sensitive to outliers.\n",
    "* **Use Case:** When large errors should be penalized more (e.g., finance, engineering).\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Root Mean Squared Error (RMSE)**\n",
    "\n",
    "* **Definition:** Square root of MSE, bringing it back to the **same units as the target variable**.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "  $$\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Like MSE, penalizes large errors.\n",
    "  * Easier to interpret than MSE because units match the target variable.\n",
    "* **Use Case:** General-purpose regression evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. R-squared (R² Score / Coefficient of Determination)**\n",
    "\n",
    "* **Definition:** Measures the **proportion of variance in the dependent variable** explained by the model.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}*i)^2}{\\sum*{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "  $$\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Value between **0 and 1** (sometimes negative for very poor models).\n",
    "  * Higher R² → better fit.\n",
    "* **Use Case:** To check **overall goodness of fit** of the regression model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Adjusted R-squared**\n",
    "\n",
    "* **Definition:** Modified R² that **adjusts for the number of features** in the model.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1-R^2)(n-1)}{n-p-1} \\right)\n",
    "  $$\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Penalizes adding irrelevant features.\n",
    "  * Better than R² for multiple regression.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "* **Definition:** Measures the **average percentage error** between predicted and actual values.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{|y_i|}\n",
    "  $$\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Expresses error as a **percentage**, making it easier to interpret.\n",
    "  * Sensitive if actual values are close to zero.\n",
    "* **Use Case:** Forecasting sales, demand prediction, financial predictions.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Takeaways**\n",
    "\n",
    "* Regression predicts **continuous numerical values**.\n",
    "* Used when **output is quantitative**, not categorical.\n",
    "* Linear regression assumes a **linear relationship**, while polynomial or tree-based models handle non-linear patterns.\n",
    "* **MAE** → Simple average error; less sensitive to outliers.\n",
    "* **MSE/RMSE** → Penalizes larger errors; RMSE in same units as target.\n",
    "* **R² / Adjusted R²** → Measures overall fit; adjusted R² for multiple predictors.\n",
    "* **MAPE** → Percentage-based, intuitive for business applications.\n",
    "* **Choice of metric** depends on the **problem context and sensitivity to large errors**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0449d4d",
   "metadata": {},
   "source": [
    "### **2.1.2 Classification in Machine Learning**\n",
    "\n",
    "### **Definition**\n",
    "\n",
    "Classification is a type of **Supervised Learning** where the model predicts **categorical (discrete) outcomes** based on input features.\n",
    "\n",
    "* **Input → Discrete Output (Class Labels)**\n",
    "* Goal: Assign each input to one of the **predefined classes**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Classification Works**\n",
    "\n",
    "1. **Collect labeled data** – Inputs with corresponding class labels.\n",
    "2. **Split data** – Training and testing sets.\n",
    "3. **Choose a classification algorithm** – E.g., Logistic Regression, Decision Tree, Random Forest.\n",
    "4. **Train the model** – Learn patterns in the features for each class.\n",
    "5. **Predict on new data** – Assign class labels to unseen instances.\n",
    "6. **Evaluate performance** – Using metrics like accuracy, precision, recall, F1-score, ROC-AUC.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Classification**\n",
    "\n",
    "| Type                           | Description                                  | Example                                            |\n",
    "| ------------------------------ | -------------------------------------------- | -------------------------------------------------- |\n",
    "| **Binary Classification**      | Two possible classes                         | Spam vs. Not Spam, Disease vs. Healthy             |\n",
    "| **Multi-class Classification** | More than two classes                        | Predicting animal type: Cat, Dog, Rabbit           |\n",
    "| **Multi-label Classification** | Each instance can belong to multiple classes | Tagging a photo with “Beach”, “Sunset”, “Vacation” |\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Classification Algorithms**\n",
    "\n",
    "| Algorithm                        | Type              | Description                                       | Example Use Case                         |\n",
    "| -------------------------------- | ----------------- | ------------------------------------------------- | ---------------------------------------- |\n",
    "| **Logistic Regression**          | Binary/Multiclass | Predicts probability of class membership          | Spam detection, medical diagnosis        |\n",
    "| **Decision Tree**                | Binary/Multiclass | Tree-based splits to classify instances           | Customer churn prediction                |\n",
    "| **Random Forest**                | Binary/Multiclass | Ensemble of decision trees for robust predictions | Fraud detection, loan approval           |\n",
    "| **Support Vector Machine (SVM)** | Binary/Multiclass | Finds best separating hyperplane between classes  | Handwriting recognition                  |\n",
    "| **K-Nearest Neighbors (KNN)**    | Binary/Multiclass | Classifies based on closest neighbors             | Recommender systems, credit scoring      |\n",
    "| **Naive Bayes**                  | Binary/Multiclass | Probability-based using Bayes theorem             | Email spam filtering, sentiment analysis |\n",
    "| **Gradient Boosting / XGBoost**  | Binary/Multiclass | Boosted tree ensembles for higher accuracy        | Risk prediction, competition datasets    |\n",
    "| **Neural Networks (MLP, CNN)**   | Binary/Multiclass | Learns complex patterns for classification        | Image recognition, speech recognition    |\n",
    "\n",
    "---\n",
    "\n",
    "### **Examples of Classification**\n",
    "\n",
    "| Domain            | Problem                            | Algorithm Example                   |\n",
    "| ----------------- | ---------------------------------- | ----------------------------------- |\n",
    "| Healthcare        | Predict if a patient has a disease | Logistic Regression, Random Forest  |\n",
    "| Finance           | Predict loan approval (yes/no)     | Decision Tree, Gradient Boosting    |\n",
    "| Marketing         | Predict if a customer will churn   | SVM, Random Forest                  |\n",
    "| Retail            | Predict product category           | KNN, Naive Bayes                    |\n",
    "| Image Recognition | Classify handwritten digits        | CNN (Convolutional Neural Networks) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Metrics for Classification**\n",
    "\n",
    "Classification metrics are based on the **confusion matrix**, which summarizes predictions vs. actual labels.\n",
    "\n",
    "#### **Confusion Matrix**\n",
    "\n",
    "|                     | Predicted Positive  | Predicted Negative  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Accuracy**\n",
    "\n",
    "* **Definition:** Fraction of total correct predictions.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  $$\n",
    "* **Key Point:** Easy to interpret, but can be misleading for **imbalanced datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Precision**\n",
    "\n",
    "* **Definition:** Fraction of correctly predicted positive instances among all predicted positives.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "* **Key Point:** Focuses on **minimizing false positives**.\n",
    "* **Use Case:** Spam detection, fraud detection.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Recall (Sensitivity / True Positive Rate)**\n",
    "\n",
    "* **Definition:** Fraction of correctly predicted positive instances among all actual positives.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "* **Key Point:** Focuses on **minimizing false negatives**.\n",
    "* **Use Case:** Medical diagnosis, cancer detection.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. F1-Score**\n",
    "\n",
    "* **Definition:** Harmonic mean of precision and recall.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "* **Key Point:** Balances precision and recall; useful for **imbalanced datasets**.\n",
    "* **Use Case:** Fraud detection, rare event prediction.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Specificity (True Negative Rate)**\n",
    "\n",
    "* **Definition:** Fraction of correctly predicted negative instances among all actual negatives.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "  $$\n",
    "* **Key Point:** Measures ability to detect negatives.\n",
    "* **Use Case:** When avoiding false alarms is critical.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. ROC-AUC (Receiver Operating Characteristic – Area Under Curve)**\n",
    "\n",
    "**Definition**\n",
    "\n",
    "* **ROC (Receiver Operating Characteristic) Curve:** A plot that shows the **trade-off between True Positive Rate (TPR / Recall) and False Positive Rate (FPR)** at different classification thresholds.\n",
    "* **AUC (Area Under the Curve):** A single scalar value summarizing the **overall ability of the classifier to distinguish between classes**.\n",
    "\n",
    "#### **Key Concepts**\n",
    "\n",
    "1. **True Positive Rate (TPR / Recall / Sensitivity):**\n",
    "   $$\n",
    "   TPR = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "\n",
    "* Measures proportion of actual positives correctly identified.\n",
    "\n",
    "2. **False Positive Rate (FPR):**\n",
    "   $$\n",
    "   FPR = \\frac{FP}{FP + TN}\n",
    "   $$\n",
    "\n",
    "* Measures proportion of actual negatives incorrectly classified as positive.\n",
    "\n",
    "3. **ROC Curve:**\n",
    "\n",
    "* X-axis: FPR (0 → 1)\n",
    "* Y-axis: TPR (0 → 1)\n",
    "* Each point corresponds to a **different threshold** used to classify probabilities into classes.\n",
    "\n",
    "4. **AUC:**\n",
    "\n",
    "* Value between **0 and 1**:\n",
    "\n",
    "  * **0.5 → random guessing**\n",
    "  * **1.0 → perfect classifier**\n",
    "  * **<0.5 → worse than random** (rare, indicates reversed predictions)\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "| AUC Value | Interpretation       |\n",
    "| --------- | -------------------- |\n",
    "| 0.9 – 1.0 | Excellent classifier |\n",
    "| 0.8 – 0.9 | Good classifier      |\n",
    "| 0.7 – 0.8 | Fair classifier      |\n",
    "| 0.6 – 0.7 | Poor classifier      |\n",
    "| 0.5       | Random guessing      |\n",
    "\n",
    "\n",
    "**Connection to Classification Metrics**\n",
    "\n",
    "* **Type 1 Error (FP) vs Type 2 Error (FN):**\n",
    "\n",
    "  * ROC curve visualizes the **trade-off between TPR (minimizing FN) and FPR (minimizing FP)**.\n",
    "* **Threshold-independent evaluation:**\n",
    "\n",
    "  * Unlike accuracy, precision, or recall which depend on a specific threshold, **AUC evaluates classifier performance across all thresholds**.\n",
    "* **Useful for imbalanced datasets:**\n",
    "\n",
    "  * Helps assess **discriminative ability** even when one class is much smaller.\n",
    "\n",
    "\n",
    "**Advantages of AUC-ROC**\n",
    "\n",
    "1. **Threshold-independent** – evaluates model across all possible thresholds.\n",
    "2. **Class imbalance friendly** – focuses on ranking positives higher than negatives.\n",
    "3. **Interpretability** – probability that a randomly chosen positive is ranked higher than a randomly chosen negative.\n",
    "\n",
    "\n",
    "**Visualization Example**\n",
    "\n",
    "* **ROC Curve:**\n",
    "\n",
    "  * X-axis = FPR\n",
    "  * Y-axis = TPR\n",
    "  * Curve above diagonal = better than random.\n",
    "  * **AUC** = area under the curve.\n",
    "\n",
    "```\n",
    "TPR |\n",
    "    |       *\n",
    "    |      *\n",
    "    |     *\n",
    "    |    *\n",
    "    |   *\n",
    "    |__*_____________ FPR\n",
    "       0   0.5    1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Log Loss (Cross-Entropy Loss)**\n",
    "\n",
    "* **Definition:** Measures how close the predicted probabilities are to the true class labels.\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_{i,j} \\log(p_{i,j})\n",
    "  $$\n",
    "* **Key Point:** Penalizes wrong confident predictions.\n",
    "* **Use Case:** Probabilistic classification, logistic regression.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table of Classification Metrics**\n",
    "\n",
    "| Metric                   | Formula                                   | Focus                             | Best Use Case                             |\n",
    "| ------------------------ | ----------------------------------------- | --------------------------------- | ----------------------------------------- |\n",
    "| **Accuracy**             | ((TP+TN)/(TP+TN+FP+FN))                   | Overall correctness               | Balanced datasets                         |\n",
    "| **Precision**            | (TP/(TP+FP))                              | Avoid false positives             | Spam detection, fraud detection           |\n",
    "| **Recall (Sensitivity)** | (TP/(TP+FN))                              | Avoid false negatives             | Medical diagnosis, disease detection      |\n",
    "| **F1-Score**             | (2*(Precision*Recall)/(Precision+Recall)) | Balance Precision & Recall        | Imbalanced datasets                       |\n",
    "| **Specificity**          | (TN/(TN+FP))                              | True negative detection           | False alarm sensitive tasks               |\n",
    "| **ROC-AUC**              | Area under ROC curve                      | Class separation ability          | Model comparison for imbalance            |\n",
    "| **Log Loss**             | Cross-entropy loss                        | Probabilistic prediction accuracy | Logistic regression, probabilistic models |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Type 1 & Type 2 Errors in Classification**\n",
    "\n",
    "In classification, we have a **confusion matrix**:\n",
    "\n",
    "|                  | Predicted Positive | Predicted Negative |\n",
    "|------------------|-----------------|-----------------|\n",
    "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Type 1 Error (False Positive)**\n",
    "\n",
    "- **Definition:** Model predicts **Positive**, but actual is **Negative**.  \n",
    "- **ML Equivalent:** **False Positive (FP)**  \n",
    "- **Example:**  \n",
    "  - Medical Diagnosis: Model predicts “disease” → patient is actually healthy.  \n",
    "  - Spam Filter: Marks a legitimate email as spam.  \n",
    "\n",
    "- **Relevant Classification Metrics:**  \n",
    "  | Metric | Connection to Type 1 Error | Interpretation |\n",
    "  |--------|---------------------------|----------------|\n",
    "  | **Precision** | TP / (TP + FP) → FP appears in denominator | High FP → Low precision |\n",
    "  | **Specificity (True Negative Rate)** | TN / (TN + FP) → FP appears in denominator | High FP → Low specificity |\n",
    "  | **FPR (False Positive Rate)** | FP / (FP + TN) | Measures proportion of negative instances misclassified as positive |\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Type 2 Error (False Negative)**\n",
    "\n",
    "- **Definition:** Model predicts **Negative**, but actual is **Positive**.  \n",
    "- **ML Equivalent:** **False Negative (FN)**  \n",
    "- **Example:**  \n",
    "  - Medical Diagnosis: Model predicts “healthy” → patient actually has disease.  \n",
    "  - Spam Filter: Marks a spam email as not spam.  \n",
    "\n",
    "- **Relevant Classification Metrics:**  \n",
    "  | Metric | Connection to Type 2 Error | Interpretation |\n",
    "  |--------|---------------------------|----------------|\n",
    "  | **Recall (Sensitivity / True Positive Rate)** | TP / (TP + FN) → FN in denominator | High FN → Low recall |\n",
    "  | **FNR (False Negative Rate)** | FN / (TP + FN) | Measures proportion of positives missed by the model |\n",
    "  | **F1-Score** | Combines precision and recall | High FN reduces F1-score |\n",
    "\n",
    "- **Quick Mapping Table**\n",
    "\n",
    "| ML Term | Type of Error | Confusion Matrix | Metric Impact |\n",
    "|---------|--------------|----------------|---------------|\n",
    "| **Type 1 Error** | False Positive | Predicted Positive, Actual Negative | Precision ↓, Specificity ↓, FPR ↑ |\n",
    "| **Type 2 Error** | False Negative | Predicted Negative, Actual Positive | Recall ↓, FNR ↑, F1-Score ↓ |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "1. **Accuracy** is simple but not reliable for **imbalanced datasets**.\n",
    "2. **Precision** = focus on correctness of positive predictions.\n",
    "3. **Recall** = focus on capturing all positives.\n",
    "4. **F1-score** balances **precision and recall**; essential for imbalanced data.\n",
    "5. **ROC-AUC** measures overall discrimination capability of the model.\n",
    "6. **Log Loss** evaluates probabilistic predictions, penalizing confident wrong guesses.\n",
    "7. **Type 1 Error (FP):** Predict positive incorrectly → affects **precision and specificity**.  \n",
    "8. **Type 2 Error (FN):** Predict negative incorrectly → affects **recall and F1-score**.  \n",
    "9. **Trade-off:** Reducing Type 1 (FP) usually increases Type 2 (FN), and vice versa.  \n",
    "10. **Choice depends on context:**  \n",
    "   - Medical tests → minimize **Type 2 errors** (don’t miss sick patients).  \n",
    "   - Spam detection → minimize **Type 1 errors** (don’t mark good emails as spam). \n",
    "11. Classification predicts **categorical outcomes**.\n",
    "12. Can be **binary, multi-class, or multi-label**.\n",
    "13. **Algorithm choice** depends on data size, feature types, and problem complexity.\n",
    "14. Evaluation must consider **class imbalance**; metrics like F1-score or ROC-AUC may be more meaningful than accuracy.\n",
    "15. Common applications include **spam detection, fraud detection, medical diagnosis, image recognition, and customer churn prediction**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8fafc",
   "metadata": {},
   "source": [
    "### **2.2.1 Clustering in Machine Learning**\n",
    "\n",
    "### **Definition**\n",
    "\n",
    "Clustering is an **unsupervised learning** technique that groups data points into **clusters** such that:\n",
    "\n",
    "* Data points within the **same cluster are similar**\n",
    "* Data points in **different clusters are dissimilar**\n",
    "\n",
    "👉 There are **no labels**; the model discovers structure automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Clustering Works**\n",
    "\n",
    "1. Collect **unlabeled data**\n",
    "2. Choose a **similarity/distance measure** (e.g., Euclidean distance)\n",
    "3. Apply a **clustering algorithm**\n",
    "4. Algorithm groups data into clusters\n",
    "5. Analyze clusters for insights or downstream tasks\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Clustering**\n",
    "\n",
    "| Type                | Description                                        | Example                      |\n",
    "| ------------------- | -------------------------------------------------- | ---------------------------- |\n",
    "| **Partition-based** | Divides data into K non-overlapping clusters       | K-Means                      |\n",
    "| **Hierarchical**    | Creates a tree of clusters (dendrogram)            | Agglomerative Clustering     |\n",
    "| **Density-based**   | Groups dense regions, handles noise                | DBSCAN                       |\n",
    "| **Model-based**     | Assumes data comes from a probability distribution | Gaussian Mixture Model (GMM) |\n",
    "| **Grid-based**      | Divides data into grid structures                  | STING                        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Clustering Algorithms**\n",
    "\n",
    "| Algorithm                        | Type            | Description                             | Use Case                         |\n",
    "| -------------------------------- | --------------- | --------------------------------------- | -------------------------------- |\n",
    "| **K-Means**                      | Partition-based | Minimizes distance to cluster centroids | Customer segmentation            |\n",
    "| **Hierarchical Clustering**      | Hierarchical    | Builds cluster tree (dendrogram)        | Gene analysis                    |\n",
    "| **DBSCAN**                       | Density-based   | Finds dense clusters & noise            | Fraud detection, geospatial data |\n",
    "| **Gaussian Mixture Model (GMM)** | Model-based     | Soft clustering using probabilities     | Image segmentation               |\n",
    "| **Mean Shift**                   | Density-based   | Shifts points toward density peaks      | Object tracking                  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Distance Measures Used in Clustering**\n",
    "\n",
    "| Distance Metric        | Description                    |\n",
    "| ---------------------- | ------------------------------ |\n",
    "| **Euclidean Distance** | Straight-line distance         |\n",
    "| **Manhattan Distance** | Sum of absolute differences    |\n",
    "| **Cosine Similarity**  | Measures angle between vectors |\n",
    "| **Jaccard Distance**   | Similarity between sets        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Examples of Clustering Applications**\n",
    "\n",
    "| Domain           | Problem                   | Algorithm               |\n",
    "| ---------------- | ------------------------- | ----------------------- |\n",
    "| Retail           | Customer segmentation     | K-Means                 |\n",
    "| Finance          | Fraud detection           | DBSCAN                  |\n",
    "| Healthcare       | Disease pattern discovery | Hierarchical Clustering |\n",
    "| Marketing        | Market segmentation       | K-Means, GMM            |\n",
    "| Image Processing | Image compression         | K-Means                 |\n",
    "| Social Media     | User grouping             | DBSCAN                  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Clustering Evaluation Metrics**\n",
    "\n",
    "Since clustering is **unsupervised**, we usually **don’t have true labels**. Therefore, clustering evaluation is divided into:\n",
    "\n",
    "1. **Internal Metrics** – use only the data & clusters\n",
    "2. **External Metrics** – use true labels (if available)\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Internal Clustering Evaluation Metrics (Most Common)**\n",
    "\n",
    "#### **1. Silhouette Score**\n",
    "\n",
    "* **Definition:** Measures how similar a point is to its own cluster compared to other clusters.\n",
    "\n",
    "* **Formula:**\n",
    "  $$\n",
    "  s = \\frac{b - a}{\\max(a, b)}\n",
    "  $$\n",
    "  Where:\n",
    "\n",
    "* `a` = average distance to points in same cluster\n",
    "\n",
    "* `b` = average distance to points in nearest cluster\n",
    "\n",
    "* **Range:** `-1 to +1`\n",
    "\n",
    "| Value       | Meaning              |\n",
    "| ----------- | -------------------- |\n",
    "| Close to +1 | Well-clustered       |\n",
    "| Around 0    | Overlapping clusters |\n",
    "| Negative    | Wrong clustering     |\n",
    "\n",
    "✔ **Best Metric for:** Overall clustering quality\n",
    "✔ **Used with:** K-Means, Hierarchical clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Davies–Bouldin Index (DBI)**\n",
    "\n",
    "* **Definition:** Measures **average similarity between each cluster and its most similar cluster**.\n",
    "* **Key Idea:** Lower is better.\n",
    "* **Range:** `0 → ∞`\n",
    "\n",
    "| Value | Meaning                           |\n",
    "| ----- | --------------------------------- |\n",
    "| Low   | Compact & well-separated clusters |\n",
    "| High  | Poor clustering                   |\n",
    "\n",
    "✔ **Best Metric for:** Comparing multiple clustering models\n",
    "✔ **Sensitive to:** Cluster shape\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Calinski–Harabasz Index (CHI)**\n",
    "\n",
    "* **Definition:** Ratio of **between-cluster dispersion to within-cluster dispersion**.\n",
    "* **Key Idea:** Higher is better.\n",
    "\n",
    "✔ **Best Metric for:** Finding optimal number of clusters\n",
    "✔ **Works well with:** K-Means\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Elbow Method (Inertia / WCSS)**\n",
    "\n",
    "* **Definition:** Measures **within-cluster sum of squares**.\n",
    "* **Approach:** Plot WCSS vs number of clusters (K).\n",
    "\n",
    "✔ **Optimal K:** Point where curve bends (“elbow”)\n",
    "\n",
    "✔ **Limitation:** Subjective interpretation\n",
    "✔ **Used only for:** K-Means\n",
    "\n",
    "---\n",
    "\n",
    "### **2. External Clustering Evaluation Metrics (If Labels Exist)**\n",
    "\n",
    "#### **5. Adjusted Rand Index (ARI)**\n",
    "\n",
    "* **Definition:** Measures similarity between true labels and predicted clusters.\n",
    "* **Range:** `-1 to 1`\n",
    "\n",
    "| Value | Meaning           |\n",
    "| ----- | ----------------- |\n",
    "| 1     | Perfect match     |\n",
    "| 0     | Random clustering |\n",
    "| <0    | Worse than random |\n",
    "\n",
    "✔ **Best Metric for:** Ground-truth comparison\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Normalized Mutual Information (NMI)**\n",
    "\n",
    "* **Definition:** Measures information shared between true labels and clusters.\n",
    "* **Range:** `0 to 1`\n",
    "\n",
    "✔ **Insensitive to:** Number of clusters\n",
    "✔ **Used in:** NLP & image clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Homogeneity, Completeness & V-Measure**\n",
    "\n",
    "| Metric           | Meaning                              |\n",
    "| ---------------- | ------------------------------------ |\n",
    "| **Homogeneity**  | Each cluster contains only one class |\n",
    "| **Completeness** | All class samples in same cluster    |\n",
    "| **V-Measure**    | Harmonic mean of both                |\n",
    "\n",
    "✔ **Best for:** Evaluating label consistency\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table**\n",
    "\n",
    "| Metric            | Type     | Best Value  | When to Use                |\n",
    "| ----------------- | -------- | ----------- | -------------------------- |\n",
    "| Silhouette Score  | Internal | ↑ High      | General clustering quality |\n",
    "| Davies–Bouldin    | Internal | ↓ Low       | Model comparison           |\n",
    "| Calinski–Harabasz | Internal | ↑ High      | Optimal K selection        |\n",
    "| Elbow Method      | Internal | Elbow point | K-Means only               |\n",
    "| ARI               | External | ↑ High      | True labels available      |\n",
    "| NMI               | External | ↑ High      | Label comparison           |\n",
    "| V-Measure         | External | ↑ High      | Class consistency          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "1. **No labels → use internal metrics** (Silhouette, DBI, CHI)\n",
    "2. **Labels available → use external metrics** (ARI, NMI)\n",
    "3. **Silhouette Score** is the most widely used metric\n",
    "4. **DBI ↓ lower is better**, **CHI ↑ higher is better**\n",
    "5. **Elbow method helps choose K**, but is subjective\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Clustering**\n",
    "\n",
    "* Works without labeled data\n",
    "* Helps discover hidden patterns\n",
    "* Useful for exploratory data analysis (EDA)\n",
    "* Scales to large datasets\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "* Choosing the right number of clusters can be difficult\n",
    "* Sensitive to noise and outliers (e.g., K-Means)\n",
    "* Results depend on distance metrics\n",
    "* Interpretation may be subjective\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "1. Clustering is an **unsupervised learning technique**.\n",
    "2. Used to **group similar data points**.\n",
    "3. Common algorithms: **K-Means, Hierarchical, DBSCAN, GMM**.\n",
    "4. Evaluation uses **internal metrics** like silhouette score.\n",
    "5. Widely used in **customer segmentation, fraud detection, image processing**, and more.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50413a9f",
   "metadata": {},
   "source": [
    "### **2.2.2 Dimensionality Reduction**\n",
    "\n",
    "#### **Definition**\n",
    "\n",
    "Dimensionality Reduction is an **unsupervised learning technique** used to **reduce the number of input features (dimensions)** in a dataset while **preserving as much important information as possible**.\n",
    "\n",
    "👉 Goal: **Simpler data, faster models, less noise, better visualization**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Dimensionality Reduction is Needed**\n",
    "\n",
    "* High-dimensional data → **curse of dimensionality**\n",
    "* Reduces **overfitting**\n",
    "* Improves **training speed**\n",
    "* Removes **redundant and noisy features**\n",
    "* Enables **2D/3D visualization**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Types of Dimensionality Reduction**\n",
    "\n",
    "| Type                   | Description                                            |\n",
    "| ---------------------- | ------------------------------------------------------ |\n",
    "| **Feature Selection**  | Selects a subset of original features                  |\n",
    "| **Feature Extraction** | Transforms features into a new lower-dimensional space |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Common Dimensionality Reduction Algorithms**\n",
    "\n",
    "#### **1. Principal Component Analysis (PCA)**\n",
    "\n",
    "* **Type:** Linear, Feature Extraction\n",
    "* **Idea:** Projects data onto directions of **maximum variance**\n",
    "* **Output:** Principal Components (orthogonal axes)\n",
    "\n",
    "✔ Fast & widely used\n",
    "❌ Assumes linearity\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Linear Discriminant Analysis (LDA)**\n",
    "\n",
    "* **Type:** Supervised dimensionality reduction\n",
    "* **Idea:** Maximizes **class separability**\n",
    "* **Used when:** Labels are available\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "\n",
    "* **Type:** Non-linear\n",
    "* **Idea:** Preserves **local structure** for visualization\n",
    "* **Used for:** 2D/3D visualization\n",
    "\n",
    "✔ Excellent visualization\n",
    "❌ Not scalable, not for modeling\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. UMAP (Uniform Manifold Approximation and Projection)**\n",
    "\n",
    "* **Type:** Non-linear\n",
    "* **Idea:** Preserves both **local and global structure**\n",
    "\n",
    "✔ Faster & scalable than t-SNE\n",
    "✔ Good for large datasets\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Autoencoders**\n",
    "\n",
    "* **Type:** Deep Learning-based\n",
    "* **Idea:** Neural network compresses and reconstructs data\n",
    "\n",
    "✔ Handles complex non-linear data\n",
    "❌ Requires more data & compute\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm Comparison Table**\n",
    "\n",
    "| Algorithm   | Linear | Supervised | Best For                    |\n",
    "| ----------- | ------ | ---------- | --------------------------- |\n",
    "| PCA         | Yes    | No         | Noise reduction, speed      |\n",
    "| LDA         | Yes    | Yes        | Class separability          |\n",
    "| t-SNE       | No     | No         | Visualization               |\n",
    "| UMAP        | No     | No         | Visualization + structure   |\n",
    "| Autoencoder | No     | No         | Complex data (images, text) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "| Domain          | Use Case                                       |\n",
    "| --------------- | ---------------------------------------------- |\n",
    "| Computer Vision | Image compression                              |\n",
    "| NLP             | Word embeddings                                |\n",
    "| Healthcare      | Gene expression analysis                       |\n",
    "| Finance         | Feature reduction                              |\n",
    "| ML Pipelines    | Preprocessing before clustering/classification |\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "\n",
    "Dimensionality reduction does **not directly predict labels**, so evaluation focuses on:\n",
    "\n",
    "1. **Information preservation**\n",
    "2. **Reconstruction quality**\n",
    "3. **Structure preservation**\n",
    "4. **Downstream task performance**\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Information Preservation Metrics**\n",
    "\n",
    "**Explained Variance Ratio (PCA)**\n",
    "\n",
    "* **Definition:** Measures how much variance (information) is retained after reduction.\n",
    "\n",
    "* **Formula:**\n",
    "  $$\n",
    "  \\text{Explained Variance Ratio} = \\frac{\\text{Variance of selected components}}{\\text{Total variance}}\n",
    "  $$\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * 95% variance → very good\n",
    "  * 80–90% → acceptable\n",
    "\n",
    "✔ **Used with:** PCA\n",
    "✔ **Goal:** Retain maximum information\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Reconstruction-Based Metrics**\n",
    "\n",
    " **Reconstruction Error**\n",
    "\n",
    "* **Definition:** Measures difference between original data and reconstructed data.\n",
    "* **Common measures:**\n",
    "\n",
    "  * Mean Squared Error (MSE)\n",
    "  * Mean Absolute Error (MAE)\n",
    "\n",
    "$$\n",
    "\\text{Reconstruction Error} = ||X - \\hat{X}||^2\n",
    "$$\n",
    "\n",
    "✔ **Lower = better**\n",
    "✔ **Used with:** PCA, Autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Structure Preservation Metrics**\n",
    "\n",
    "**Trustworthiness**\n",
    "\n",
    "* **Definition:** Measures whether **nearest neighbors in reduced space** were neighbors in original space.\n",
    "* **Range:** 0 to 1\n",
    "\n",
    "✔ **High value → good local structure**\n",
    "\n",
    "---\n",
    "\n",
    "**Continuity**\n",
    "\n",
    "* **Definition:** Measures whether **original neighbors remain neighbors** after reduction.\n",
    "* **Range:** 0 to 1\n",
    "\n",
    "✔ **High value → minimal structure loss**\n",
    "\n",
    "---\n",
    "\n",
    "**Mean Relative Rank Error (MRRE)**\n",
    "\n",
    "* Measures ranking distortion between high-dimensional and low-dimensional spaces.\n",
    "\n",
    "✔ **Lower = better**\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Visualization-Oriented Metrics**\n",
    "\n",
    "**Silhouette Score (After Reduction)**\n",
    "\n",
    "* Apply clustering on reduced data\n",
    "* Measures **cluster separation**\n",
    "\n",
    "✔ Useful when DR is followed by clustering\n",
    "✔ Higher score → better separation\n",
    "\n",
    "---\n",
    "\n",
    "**KNN Preservation Score**\n",
    "\n",
    "* Measures how many nearest neighbors are preserved after reduction.\n",
    "\n",
    "✔ Especially useful for **t-SNE & UMAP**\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Downstream Task Performance**\n",
    "\n",
    "**Classification / Clustering Performance**\n",
    "\n",
    "* Train a model on reduced data\n",
    "* Evaluate using:\n",
    "\n",
    "  * Accuracy\n",
    "  * F1-score\n",
    "  * AUC\n",
    "  * Silhouette Score\n",
    "\n",
    "✔ Best real-world evaluation method\n",
    "\n",
    "---\n",
    "\n",
    "## **Metric vs Algorithm Mapping**\n",
    "\n",
    "| Metric                | PCA | t-SNE | UMAP | LDA | Autoencoder |\n",
    "| --------------------- | --- | ----- | ---- | --- | ----------- |\n",
    "| Explained Variance    | ✅   | ❌     | ❌    | ❌   | ❌           |\n",
    "| Reconstruction Error  | ✅   | ❌     | ❌    | ❌   | ✅           |\n",
    "| Trustworthiness       | ⚠️  | ✅     | ✅    | ⚠️  | ✅           |\n",
    "| Continuity            | ⚠️  | ✅     | ✅    | ⚠️  | ✅           |\n",
    "| Downstream Accuracy   | ✅   | ⚠️    | ⚠️   | ✅   | ✅           |\n",
    "| Visualization Quality | ❌   | ✅     | ✅    | ⚠️  | ⚠️          |\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison Table**\n",
    "\n",
    "| Metric               | What It Measures          | Best For          |\n",
    "| -------------------- | ------------------------- | ----------------- |\n",
    "| Explained Variance   | Information retained      | PCA               |\n",
    "| Reconstruction Error | Data loss                 | PCA, Autoencoders |\n",
    "| Trustworthiness      | Local structure           | t-SNE, UMAP       |\n",
    "| Continuity           | Neighborhood preservation | t-SNE, UMAP       |\n",
    "| Silhouette Score     | Cluster separation        | DR + Clustering   |\n",
    "| Downstream Accuracy  | Real usefulness           | All DR methods    |\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "* Reduces complexity\n",
    "* Improves model performance\n",
    "* Helps visualization\n",
    "* Reduces storage & computation\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "* Possible loss of information\n",
    "* Harder interpretation (feature extraction)\n",
    "* t-SNE & UMAP not ideal for modeling\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "1. Dimensionality reduction simplifies high-dimensional data\n",
    "2. **PCA** is the most common linear method\n",
    "3. **t-SNE & UMAP** are best for visualization\n",
    "4. **LDA** uses labels to maximize class separation\n",
    "5. **Autoencoders** handle complex, non-linear data\n",
    "6. **No single metric fits all DR methods**\n",
    "7. **PCA → Explained variance & reconstruction error**\n",
    "8. **t-SNE / UMAP → Trustworthiness & continuity**\n",
    "9. **Autoencoders → Reconstruction loss**\n",
    "10. **Best evaluation → downstream task performance**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d60e7",
   "metadata": {},
   "source": [
    "# <center> *End of Topic* </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "note_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
